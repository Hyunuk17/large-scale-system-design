> **6장. 키-값 저장소 설계**

<details>
  <summary><b>문제 이해 및 설계 범위 확정</b></summary>
  
  ---
    
  ## 문제 이해 및 설계 범위 확정
  
  ### **키-값 저장소(Key-Value Store)**
  
  - **비 관계형(Non-relational) 데이터베이스**
      - Amazon Dynamo, memcached, Redis 등
  - **Key**
      - 고유 식별자(Identifier)
      - 성능상의 이유로 키는 짧을수록 좋음
          - 일반 텍스트 키: `“last_logged_in_at”`
          - 해시 키 : `253DDEC4`
  - **Value**
      - 키-값 저장소는 값으로 무엇이 오든 상관하지 않음(`String`, `List`, `Object` …)
  - **Pair** : Key-Value 관계
  - `put(key, value)` : 키-값 쌍을 저장소에 저장
  - `get(key)` : 인자로 주어진 키의 값을 조회
  
  ### **문제 이해 및 설계 범위 확정**
  
  - 읽기, 쓰기, 메모리 사용량 사이의 균형 찾기
  - 데이터의 일관성과 가요성 사이에서 타협적 결정을 내린 설계
  - **ex)**
      - 키-값 쌍의 **크기는 10KB 이하**이다.
      - **큰 데이터**를 저장할 수 있어야 한다.
      - 높은 **가용성을 제공**해야 한다. 시스템은 장애가 있더라도 빨리 응답
      - 높은 **규모 확장성**을 제공해야 한다. 트래픽 양에 따라 자동적 서버 증설/삭제
      - **데이터 일관성 수준**은 조정이 가능해야 한다.
      - **응답 지연시간(latency)** 이 짧아야 한다.
  
  ---
</details>

<details>
  <summary><b>단일 서버 키-값 저장소</b></summary>
    
  ---
  
  ## 단일 서버 키-값 저장소
  
  ### **서버 1대만 사용하는 키-값 저장소 설계**
  
  - **키-값 쌍 전부를 메모리에 해시 테이블로 저장**
      - 빠른 속도를 보장
      - 모든 데이터를 메모리 안에 두기 불가능할 수 있음
  - **개선책**
      - 데이터 압축(Compression)
      - 자주 쓰이는 데이터만 메모리에 두고 나머지는 디스크에 저장
  - 많은 데이터를 저장하기 위해 **분산 키-값 저장소(Distributed Key-Value Store)** 필요
  
  ---
</details>

<details>
  <summary><b>분산 키-값 저장소</b></summary>

  ---
  
  ## 분산 키-값 저장소
  
  ### **분산 키-값 저장소(Distributed Key-Value Store)**
  
  - 키-값 쌍(Pair)을 **여러 서버에 분산** : 분산 해시 테이블이라고도 함
  - 설계 시 **CAP 정리** 이해 필요
  
  ### **CAP 정리**
  
  - **세 가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는 것은 불가능**
      - **데이터 일관성(Consistency)**
          - 접속하는 모든 클라이언트는 **어떤 노드에서도 같은 데이터**를 보아야 함
      - **가용성(Availability)**
          - 접속하는 모든 클라이언트는 **일부 노드에 장애가 발생하더라도 항상 응답**
      - **파티션 감내(Partition Tolerance theorem)**
          - 파티션은 두 노드 사이에 통신 장애가 발생하였음을 의미
          - 네트워크에 **파티션이 생기더라도 시스템은 계속 동작**
  - **어떤 두 가지를 충족하려면 나머지 하나는 반드시 희생**
      
    <img src="https://github.com/user-attachments/assets/3f77f4c0-8ae4-4b64-a058-772895ba0776" width="400px"/>

      
  - **키-값 저장소 분류**
      - **CP 시스템**
          - 일관성 & 파티션 감내 지원
          - 가용성 희생
      - **AP 시스템**
          - 가용성 & 파티션 감내 지원
          - 데이터 일관성 희생
      - **CA 시스템(존재하지 않음)**
          - 일관성 & 가용성 지원
          - 파티션 감내 지원하지 않음
          - **분산 시스템은 반드시 파티션 문제를 감내할 수 있도록 설계**
              - 통상 네트워크 장애는 피할 수 없음
              - 실세계에 CA 시스템은 존재하지 않음
  
  ### **구체적 사례 : 세 대의 복제(Replica) 노드 n1, n2, n3**
  
  - 분산 시스템에서 데이터는 보통 여러 노드에 복제되어 보관
  - **이상적 상태**
      - 네트워크가 파티션되는 상황이 일어나지 않음
      - n1에 기록된 데이터는 자동적으로 n2와 n3에 복제
      - 데이터 일관성과 가용성 만족
      
    <img src="https://github.com/user-attachments/assets/51babc25-5202-4aa6-9683-3d367d4cb896" width="400px"/>

      
  - **실세계의 분산 시스템**
      - 분산 시스템은 파티션 문제를 피할 수 없음
      - 파티션 문제 발생 시 **일관성과 가용성 중 하나를 선택**해야 함
      
    <img src="https://github.com/user-attachments/assets/98fa7a0c-d192-4e0e-a02b-2311c483e51d" width="400px"/>

      
      - n1 또는 n2에 기록한 데이터는 n3에 전달되지 않음
      - n3에 기록되었으나 n1, n2에 전달되지 않은 데이터, n1, n2는 오래된 사본 가짐
      - **가용성 대신 일관성 선택(CP 시스템)**
          - 세 서버 사이의 데이터 불일치를 해결하기 위해 **n1, n2에 쓰기 연산 중단**
          - ex) 은행권 시스템은 데이터 일관성(C)을 양보하지 않음 : **오류 반환**
      - **일관성 대신 가용성 선택(AP 시스템)**
          - 낡은 데이터를 반환할 위험이 있더라도, **읽기 연산을 허용**
          - n1, n2는 계속 쓰기 연산을 허용, n3 복구 후 동기화
          
  
  ### **시스템 컴포넌트**
  
  - **키-값 저장소 구현에 사용될 핵심 컴포넌트 및 기술**
      - 데이터 파티션
      - 데이터 다중화(Replication)
      - 일관성(Consistency)
      - 일관성 불일치 해소(Inconsistency Resolution)
      - 장애 처리
      - 시스템 아키텍처 다이어그램
      - 쓰기 경로(Write Path)
      - 읽기 경로(Read Path)
      
  
  ---
  </details>

  <details>
    <summary><b>데이터 파티션</b></summary>
    
  ---
  
  ## 데이터 파티션
  
  ### **데이터 파티션**
  
  - **대규모 애플리케이션의 경우**
      - 전체 데이터를 한 대 서버에 넣기 불가능
      - 데이터를 **작은 파티션들로 분할** 후, 여러 대 서버에 저장
  - **파티션 분할 시 고려할 문제**
      - 데이터를 여러 서버에 고르게 분산할 수 있는가
      - 노드가 추가되거나 삭제될 때, 데이터의 이동을 최소화할 수 있는가
  - **안정 해시(Consistent Hash)**
      - 서버를 해시 링(Hash Ring) 위에 배치
      - 키를 링 위에 배치
      - 시계방향으로 순회하다 만나는 첫 번째 서버에 키-값 쌍 저장
          
        <img src="https://github.com/user-attachments/assets/5269a59e-b008-4976-a1e9-883c90095b36" width="400px"/>

          
  - **안정 해시를 사용한 데이터 파티션 장점**
      - **규모 확장 자동화(Automatic Scaling)**
          - 시스템 부하에 따라 서버가 자동으로 추가 / 삭제
      - **다양성(Heterogeneity)**
          - 각 서버의 용량에 맞게 가상 노드(Virtual Node)의 수를 조정할 수 있음
          - 고성능 서버는 더 많은 가상 노드를 갖도록 설정
  
  ---
  </details>

  <details>
    <summary><b>데이터 다중화</b></summary>
    
  ---
  
  ## 데이터 다중화
  
  ### **데이터 다중화**
  
  - **데이터를 N개 서버에 비동기적으로 다중화(Replication)**
      - 높은 가용성과 안정성 확보
      - N은 튜닝 가능한 값
  - **N개 서버를 선정하는 방법**
      - Key의 위치로부터 **시계방향으로 순회**하면서, **첫 N개 서버에 사본 저장**
      - 가상 노드 사용 시 문제 : **같은 물리 서버를 중복 선택**
          - 같은 데이터 센터 노드 : 정전, 네트워크 이슈, 자연재해를 동시에 겪음
          - 같은 물리 서버를 중복 선택하지 않도록 해야 함
          - 데이터의 사본은 다른 센터의 서버에 보관, 센터들은 고속 네트워크로 연결
      
    <img src="https://github.com/user-attachments/assets/3bc9e0ff-4228-4e99-94df-d1fa3539c591" width="400px"/>
  
  ---
    
  </details>

  <details>
    <summary><b>데이터 일관성</b></summary>
  
  ---
  
  ## 데이터 일관성
  
  ### **데이터 일관성**
  
  - **여러 노드에 다중화된 데이터는 적절히 동기화 필요**
  - **정족수 합의(Quorum Consensus) 프로토콜**
      - 읽기/쓰기 연산 모두에 일관성 보장 가능
      - $N$ : 사본 개수
      - $W$
          - 쓰기 연산에 대한 정족수
          - 쓰기 연산 성공으로 간주되기 위해 **적어도 $W$개 서버 쓰기 연산 성공 응답**
      - $R$
          - 읽기 연산에 대한 정족수
          - 읽기 연산 성공으로 간주되기 위해 **적어도 $R$개 서버 읽기 연산 성공** **응답**
  - **예제) $N=3$**
      
    <img src="https://github.com/user-attachments/assets/02e8fb12-932e-4c1c-929d-75a0126791a9" width="400px"/>

      
      - **중재자(Coordinator)**
          - $W=1$는 중재자가 최소 한 대 서버로부터 쓰기 성공 응답 필요로 함을 의미
          - ex) $s1$으로부터 성공 응답 시, $s2, s3$의 응답을 기다릴 필요 없음
          - 중재자는 클라이언트와 노드 사이에서 **프록시(Proxy) 역할**
      - $W=1 or R=1$
          - 중재자는 한 대 서버로부터의 응답만 받으면 되니 응답 속도가 빠름
      - $W ≥1 or R≥1$
          - 시스템이 보여주는 데이터 일관성의 수준 향상
          - 중재자의 응답 속도는 가장 느린 서버로부터의 응답을 기다려야 하므로 느려짐
      - $W+R > N$
          - **강한 일관성(Strong Consistency) 보장**
          - 일관성을 보증할 최신 데이터를 **가진 노드가 최소 하나는 겹침**
  - **요구되는 일관성 수준에 따른 시스템의 구성**
      - $R=1, W=N$ : 빠른 읽기 연산에 최적화된 시스템
      - $W=1, R=N$ : 빠른 쓰기 연산에 최적화된 시스템
      - $W+R > N$ : 강한 일관성이 보장됨(보통 $N=3, W=R=2$)
      - $W+R ≤ N$ : 강한 일관성이 보장되지 않음
  
  ### 1️⃣ **일관성 모델(Consitency Model)**
  
  - 키-값 저장소를 설계할 때 고려해야할 요소
  - 데이터 일관성의 수준을 결정
  - **강한 일관성(Strong Consistency)**
      - 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환
      - 클라이언트는 **절대로 낡은(Out-of-date) 데이터를 보지 못함**
      - 모든 사본에 현재 쓰기 연산의 결과가 반영될 때까지 해당 데이터의 읽기/쓰기 금지
      - 고가용성 시스템에는 적합하지 않음 : 새 요청의 처리가 중단됨
  - **약한 일관성(Weak Consistency)**
      - 읽기 연산은 가장 최근에 갱신된 결과를 반환하지 못할 수 있음
  - **최종 일관성(Eventual Consistency)**
      - 약한 일관성의 한 형태
      - 갱신 결과가 결국에는 모든 사본에 반영(동기화)되는 모델
      - **쓰기 연산이 병렬적 발생** 상황에 데이터의 일관성이 깨질 수 있음 : **클라이언트가 해결**
      - DynamoDB, Cassandra
  
  ### **2️⃣ 비 일관성 해소 기법: 데이터 버저닝**
  
  - **데이터를 다중화하면 가용성 증가, 사본 간 일관성이 깨질 가능성 증가**
      - **버저닝(Versioning)**
          - 데이터 변경 시 해당 데이터의 **새 버전을 생성**
          - 각 버전의 데이터는 변경 불가능(**Immutable**)
  - **예시) 데이터 일관성이 깨지는 상황**
      
    <img src="https://github.com/user-attachments/assets/baf84eb3-6d09-4d62-86b0-f5cb324064f3" width="400px"/>

      
      - 어떤 데이터의 사본이 노드 **n1**과 **n2**에 보관
      - 데이터를 가져오려는 **서버 1**과 **서버 2**는 `get(”name”)`을 통해 `“john"`을 조회
      
    <img src="https://github.com/user-attachments/assets/68e4511d-c2ab-4df5-9e88-019df936c05f" width="400px"/>

      
      - 서버 1 :  `“name”`에 연결된 값을 `“johnSanFrancisco”`로 변경, **v1**
      - 서버 2 : `“name”`에 연결된 값을 `“johnNewYork”`으로 변경, **v2**
      - 두 **연산이 동시에** 이뤄지고 **충돌(Conflict)**하는 값이 되었음
      - 충돌 발견/해결할 버저닝 시스템이 필요 : **벡터 시계(Vector ClocK)**
  
  ### 3️⃣ **벡터 시계(Vector Clock)**
  
  - **[서버, 버전]의 순서쌍을 데이터에 매단 것**
  - 어떤 버전이 선행/후행 버전인지, 다른 버전과 충돌이 있는지 판별
  - **$D([S1, v1], [S2, v2], … [Sn, vn]$**
      - $D$ : 데이터
      - $S$ : 서버 번호
      - $v$ : 버전 카운터
  - **데이터 $D$를 서버 $Si$에 기록**
      - $[Si, vi]$가 있으면, $vi$를 증가
      - 그렇지 않으면 새 항목 $[Si, 1]$을 생성
  
  <img src="https://github.com/user-attachments/assets/3a05b221-6d9c-4e9b-aee6-4c82f11a647f" width="400px"/>

  
  1. **클라이언트가 데이터 $D1$을 기록**
      - 서버 $Sx$가 연산을 처리
      - 벡터 시계 ; $D1([Sx, 1])$
  2. **다른 클라이언트가 $D1$을 읽고 업데이트**
      - 같은 서버 $Sx$가 쓰기 연산 처리
      - 벡터 시계 : $D2([Sx, 2])$
  3. **다른 클라이언트가 $D2$를 읽고 업데이트**
      - 다른 서버 $Sy$가 쓰기 연산 처리
      - 벡터 시계 : $D3([Sx, 2], [Sy, 1])$
  4. **또 다른 클라이언트가 $D2$를 읽고 갱신**
      - 다른 서버 $Sz$가 쓰기 연산 처리
      - 백터 시계 : $D4([Sx, 2], [Sy, 1], [Sz, 1])$
  5. **어떤 클라이언트가 $D3$, $D4$를 읽음**
      - 데이터 간 충돌 확인
      - 클라이언트가 충돌 해결 후 서버 기록
      - 서버 $Sx$가 쓰기 연산 처리
      - $D5([Sx, 3], [Sy, 1], [Sz, 1])$
  - **충돌 판단**
      - 버전 $X$에 포함된 모든 구성요소 값 $≤$ 버전 $Y$에 포함된 모든 구성요소 값
      - ex)
          - $X: \$ $D([s0,1], [s1, 1])$
          - $Y: \$ $D([s0, 1], [s1, 2])$
          - $X$는 $Y$의 이전 버전, **충돌 없음**
  - **벡터 시계를 이용한 충돌 감지 / 해소 단점**
      - **충돌 관련 로직이 클라이언트에 포함되어야 함**
          - 클라이언트 구현 복잡해짐
      - **[서버, 버전] 순서쌍의 개수가 너무 빨리 증가**
          - 임계치(Threshold) 설정
          - 오래된 순서쌍을 벡터 시계에서 제거 : 버전 간 선후 관계가 정확하게 결정될 수 없음
      - Amazon DynamoDB 실제 서비스에 문제가 발견된 적 없음 : **사용해도 괜찮은 솔루션**
  
  ---
  </details>

  <details>
    <summary><b>장애 처리</b></summary>
    
  ---
  
  ## 장애 처리
  
  ### **장애 처리**
  
  - 대다수 대규모 시스템에서 장애는 흔하게 벌어지는 사건
  - 장애를 어떻게 처리할 것인지는 중요한 문제
      - **장애 감지(Failure Detection)**
      - **장애 해소(Failure Resolution)**
      
  
  ### 장애 감지
  
  - **분산 시스템에는 두 대 이상의 서버가 똑같이 서버 A의 장애를 보고해야 실제 장애로 간주**
  - **모든 노드 사이에 멀티캐스팅(Multicasting) 채널 구축**
      - 서버 장애를 감지하는 가장 손쉬운 방법
      - 서버가 많은 때는 비효율적
      
    <img src="https://github.com/user-attachments/assets/61e06ca7-7425-4869-aad9-652df20e5e1e" width="400px"/>

      
  - **가십 프로토콜(Gossip Protocol)**
      - 분산형 장애 감지(Decentralized Failure Detection) 솔루션
      - 멀티캐스팅 채널 구축 방법보다 효율적
      
    <img src="https://github.com/user-attachments/assets/8bbc8cf7-0b1a-4a56-85f0-3026cecdafa7" width="400px"/>

      
      - 각 노드는 **멤버십 목록(Membership List)**을 유지
          - 각 **멤버 ID**와 그 **박동 카운터(Heartbeat Counter)** 쌍의 목록
      - 각 노드는 주기적으로 자신의 **박동 카운터를 증가**
      - 각 노드는 무작위로 선정된 노드들에게 자기 **박동 카운터 목록을 보냄**
      - 박동 카운터 목록을 받은 노드는 **멤버십 목록을 최신 값으로 갱신**
      - 어떤 멤버의 박동 카운터값이 **지정된 시간동안 갱신되지 않는 경우**
          - 해당 멤버는 **장애(Offline) 상태**로 간주
  
  ### **일시적 장애 처리**
  
  - **가십 프로토콜로 장애를 감지한 시스템은 가용성을 보장하기 위한 조치 필요**
  - **엄격한 정족수(Strict Quorum) 접근법**
      - 읽기와 쓰기 연산을 금지
  - **느슨한 정족수(Sloppy Quorum) 접근법**
      - 건강한 서버를 해시 링에서 선택
          - **쓰기 연산을 수행할 $W$개 서버**
          - **읽기 연산을 수행할 $R$개 서버**
      - 장애 상태인 서버는 무시
  - **단서 후 임시 위탁(Hinted Handoff) 기법**
      - 네트워크나 서버 문제로 장애 상태인 서버로 가는 요청은 **다른 서버가 잠시 맡아 처리**
      - 발생한 변경사항은 해당 서버가 **복구되었을 때 일괄 반영**, 데이터 일관성 보존
      - 임시로 쓰기 연산을 처리한 서버에는 **단서(hint)**를 남겨둠
      
    <img src="https://github.com/user-attachments/assets/a690038f-e45e-445b-b060-dc094febf4ac" width="400px"/>

      
      - 장애 상태인 $s2$로의 읽기 및 쓰기 연산은 일시적으로 노드 $s3$가 처리
      - $s2$가 복구되면 $s3$는 갱신된 데이터를 $s2$로 인계
  
  ### **영구 장애 처리**
  
  - **단서 후 임시 위탁 기법 :  일시적 장애를 처리**
  - **반-엔트로피(Anti-entropy) 프로토콜 : 영구적인 노드의 장애 상태 처리**
      - 사본들을 비교하여 최신 버전으로 갱신
      - **머클 트리 :** 사본 간의 일관성 상태 탐지, 전송 테이터의 양을 줄이기 위해 사용
  - **머클 트리(Merkle Tree)**
      - **해시 트리(Hash Tree)라고 불림**
      - 각 노드에 **해시 값을 레이블**로 붙여두는 트리
          - 그 자식 노드들에 보관된 값의 해시 값(자식 노드가 Leaf 노드인 경우)
          - 자식 노드들의 레이블로부터 계산된 해시 값
      - 대규모 자료 구조의 내용을 효과적, 보안상 안전한 방법으로 **검증(Verfication)** 가능
  - **머클 트리 만들기**
      - ex) 키 공간(key Space) $1$ ~ $12$
      - 1️⃣ **키 공간을 버킷(Bucket)으로 나누기**
          
        <img src="https://github.com/user-attachments/assets/006025c0-1c44-400a-8f14-fc9c2bcd9646" width="400px"/>

          
          - ex) 4개 버킷
      - **2️⃣ 해시 값 계산**
          - 버킷에 포함된 각 키에 **균등 분포 해시(Uniform Hash) 함수** 적용
          
        <img src="https://github.com/user-attachments/assets/f0d71836-0426-4061-b5ec-280db84d6349" width="400px"/>

          
      - **3️⃣ 버킷별로 해시 값 계산 후, 해시 값을 레이블로 갖는 노드 만들기**
          
        <img src="https://github.com/user-attachments/assets/60f62ba8-798b-4d86-93b7-4f28e8f52efb" width="400px"/>

          
      - **4️⃣ 이진 트리 구성**
          - 자식 노드의 레이블로부터 새로운 해시 값 계산
          - 이진 트리를 상향식으로 구성
          
        <img src="https://github.com/user-attachments/assets/981ce756-1d50-4846-84f2-108993e20a95" width="400px"/>

          
  - **머클 트리 비교**
      - **루트(Root) 노드의 해시 값 비교**
          - 일치한다면, 두 서버는 같은 데이터
          - 다른 경우
              - 왼쪽 자식 노드 해시 값 비교
              - 오른쪽 자식 노드 해시 값 비교
              - **다른 데이터를 가지는 버킷을 찾아 동기화**
  - **장점**
      - 두 서버에 보관된 **데이터의 총량과 무관**하게 동기화
      - 동기화해야하는 데이터의 양은 **실제 존재하는 차이의 크기**에 비례
          - But, 실제 시스템의 경우 버킷 하나의 크기는 꽤 큼
          - ex) 10억(1B)개 키, 백만(1M)개 버킷 : **하나의 버킷은 $1,000$개 키 관리**
      
  
  ### **데이터 센터 장애 처리**
  
  - **다양한 이유**로 장애가 발생할 수 있음
      - 정전
      - 네트워크 장애
      - 자연재해
  - **데이터를 여러 데이터 센터에 다중화하는 것이 중요**
      - 한 데이터 센터가 완전히 망가져도 다른 데이터 센터에 보관된 데이터 이용
  
  ---
  </details>

  <details>
    <summary><b>시스템 아키텍처 다이어그램</b></summary>
    
  ---
  
  ## 시스템 아키텍처 다이어그램
  
  ### 키-값 저장소 아키텍처 다이어그램
  
  - **주요 기능**
      - 클라이언트는 키-값 저장소가 제공하는 **두 가지 단순한 API**와 통신
          - `get(key)`
          - `put(key, value)`
      - **중재자(Coordinator)** 는 클라이언트에게 **키-값 저장소에 대한 프록시(Proxy)** 역할 노드
      - 노드는 안정 해시(Consistenct Hash)의 **해시 링(Hash Ring) 위에 분포**
          
        <img src="https://github.com/user-attachments/assets/51b44a40-b49e-4679-be83-a6e23ac9845c" width=""/>

          
      - 노드를 자동으로 추가/삭제 할 수 있도록, **시스템은 완전히 분산(Decentralized)**
      - 데이터는 여러 노드에 **다중화**
      - 모든 노드가 같은 책임, **SPOF(Single Point Of Failure)는 존재하지 않음**
  - **완전히 분산된 설계를 채택**
      
    <img src="https://github.com/user-attachments/assets/b3852ca4-3e48-402a-bb74-251fabdac1d4" width="400px"/>

      
      - 모든 노드는 제시된 기능 전부를 지원해야 함
  
  ### 쓰기 경로 *with Cassandra*
  
  - **쓰기 요청이 특정 노드에 전달**
      
    <img src="https://github.com/user-attachments/assets/aa4957c9-8acc-41b3-b87e-9c08cfbc9b93" width="400px"/>

      
      - **1️⃣ 쓰기 요청이 커밋 로그(Commit Log) 파일에 기록**
      - **2️⃣ 데이터가 메모리 캐시에 기록**
      - **3️⃣ 메모리 캐시가 가득차거나 임계치에 도달하면 데이터는 디스크의 SSTable에 기록**
          - SST(Sorted-String Table)
          - <키, 값>의 순서쌍을 정렬된 리스트 형태로 관리하는 테이블
  
  ### 읽기 경로 *with Cassandra*
  
  - **읽기 요청이 특정 노드에 전달**
  - **1️⃣ 데이터가 메모리 캐시에 있는지 확인**
      
    <img src="https://github.com/user-attachments/assets/2d1fcac7-a318-4faf-a1b0-0e5b72856678" width="400px"/>

      
  - **2️⃣데이터가 메모리 캐시에 있다면 해당 데이터를 클라이언트에 반환**
  - **3️⃣ 데이터가 메모리에 없는 경우에는 디스크에서 가져옴**
      - 어느 SSTable에 찾는 키가 있는지 알아내기 위해 **블룸 필터(Bloom Filter)** 사용
      
    <img src="https://github.com/user-attachments/assets/61eafc7a-5599-4d21-be09-04f240b03563" width="400px"/>

      
      - 데이터가 메모리에 있는지 검사
      - 데이터가 메모리에 없으므로 블룸 필터 검사
      - 블룸 필터를 통해 어떤 SSTable에 키가 보관되어 있는지 확인
      - SSTable에서 데이터를 가져옴
      - 해당 데이터를 클라이언트에게 반환
  
  ---
  </details>

  <details>
    <summary><b>요약</b></summary>
  
  ---
  
  ## 요약
  
  ### **분산 키-값 저장소가 가져야 하는 기능과 구현하기 위한 기술**
  
  <img src="https://github.com/user-attachments/assets/088d8f62-01b4-418a-a253-af6bcd755cfc" width="400px"/>

  
  ---
  </details>
